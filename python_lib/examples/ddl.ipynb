{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of using STOILO for Distributed Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "/home/sermir/hse/diploma/stoilo2/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sermir/hse/diploma/stoilo2/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/391], Loss: 1.8323\n",
      "Epoch [1/10], Step [200/391], Loss: 1.5452\n",
      "Epoch [1/10], Step [300/391], Loss: 1.4370\n",
      "Epoch [2/10], Step [100/391], Loss: 1.2670\n",
      "Epoch [2/10], Step [200/391], Loss: 1.2132\n",
      "Epoch [2/10], Step [300/391], Loss: 1.1414\n",
      "Epoch [3/10], Step [100/391], Loss: 1.0639\n",
      "Epoch [3/10], Step [200/391], Loss: 1.0542\n",
      "Epoch [3/10], Step [300/391], Loss: 1.0162\n",
      "Epoch [4/10], Step [100/391], Loss: 0.9350\n",
      "Epoch [4/10], Step [200/391], Loss: 0.9221\n",
      "Epoch [4/10], Step [300/391], Loss: 0.9298\n",
      "Epoch [5/10], Step [100/391], Loss: 0.8617\n",
      "Epoch [5/10], Step [200/391], Loss: 0.8503\n",
      "Epoch [5/10], Step [300/391], Loss: 0.8433\n",
      "Epoch [6/10], Step [100/391], Loss: 0.7843\n",
      "Epoch [6/10], Step [200/391], Loss: 0.8050\n",
      "Epoch [6/10], Step [300/391], Loss: 0.7685\n",
      "Epoch [7/10], Step [100/391], Loss: 0.7379\n",
      "Epoch [7/10], Step [200/391], Loss: 0.7447\n",
      "Epoch [7/10], Step [300/391], Loss: 0.7404\n",
      "Epoch [8/10], Step [100/391], Loss: 0.6993\n",
      "Epoch [8/10], Step [200/391], Loss: 0.7080\n",
      "Epoch [8/10], Step [300/391], Loss: 0.6880\n",
      "Epoch [9/10], Step [100/391], Loss: 0.6601\n",
      "Epoch [9/10], Step [200/391], Loss: 0.6792\n",
      "Epoch [9/10], Step [300/391], Loss: 0.6569\n",
      "Epoch [10/10], Step [100/391], Loss: 0.6300\n",
      "Epoch [10/10], Step [200/391], Loss: 0.6373\n",
      "Epoch [10/10], Step [300/391], Loss: 0.6537\n",
      "Accuracy: 0.7715\n",
      "Precision: 0.7742\n",
      "Recall: 0.7715\n",
      "F1 Score: 0.7676\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Гиперпараметры\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Преобразования для обучения и теста\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Загрузка датасета CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Модель: ResNet18 без предварительной тренировки, адаптированная под 10 классов\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Оценка качества на тестовом наборе\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Вычисление метрик\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import stoilo\n",
    "from stoilo.ddl import DPBGDTrainer\n",
    "\n",
    "\n",
    "# Устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Гиперпараметры\n",
    "num_epochs = 10\n",
    "batch_size = 8192\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Преобразования для обучения и теста\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Загрузка датасета CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     import cloudpickle\n",
    "#     serial = cloudpickle.dumps(batch)\n",
    "#     from pympler import asizeof\n",
    "#     print(asizeof.asizeof(serial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sermir/hse/diploma/stoilo2/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sermir/hse/diploma/stoilo2/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747128993.441949   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1747128993.461144   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 tasks\n",
      "update norm: [('conv1.weight', 0.09699437767267227), ('bn1.weight', 0.008000008761882782), ('bn1.bias', 0.00799956452101469), ('layer1.0.conv1.weight', 0.1919887214899063), ('layer1.0.bn1.weight', 0.007999800145626068), ('layer1.0.bn1.bias', 0.00799973402172327), ('layer1.0.conv2.weight', 0.19198742508888245), ('layer1.0.bn2.weight', 0.007999852299690247), ('layer1.0.bn2.bias', 0.007998902350664139), ('layer1.1.conv1.weight', 0.19198395311832428), ('layer1.1.bn1.weight', 0.007999480701982975), ('layer1.1.bn1.bias', 0.007999789901077747), ('layer1.1.conv2.weight', 0.19198077917099), ('layer1.1.bn2.weight', 0.007999792695045471), ('layer1.1.bn2.bias', 0.007999636232852936), ('layer2.0.conv1.weight', 0.2714993357658386), ('layer2.0.bn1.weight', 0.011313462629914284), ('layer2.0.bn1.bias', 0.011313029564917088), ('layer2.0.conv2.weight', 0.38397738337516785), ('layer2.0.bn2.weight', 0.0113131208345294), ('layer2.0.bn2.bias', 0.011313123628497124), ('layer2.0.downsample.0.weight', 0.09050420671701431), ('layer2.0.downsample.1.weight', 0.011313162744045258), ('layer2.0.downsample.1.bias', 0.011313123628497124), ('layer2.1.conv1.weight', 0.3839731812477112), ('layer2.1.bn1.weight', 0.011313073337078094), ('layer2.1.bn1.bias', 0.011313043534755707), ('layer2.1.conv2.weight', 0.3839744031429291), ('layer2.1.bn2.weight', 0.01131303608417511), ('layer2.1.bn2.bias', 0.011313125491142273), ('layer3.0.conv1.weight', 0.5429621338844299), ('layer3.0.bn1.weight', 0.015998756512999535), ('layer3.0.bn1.bias', 0.015998447313904762), ('layer3.0.conv2.weight', 0.7675011157989502), ('layer3.0.bn2.weight', 0.015999216586351395), ('layer3.0.bn2.bias', 0.015998905524611473), ('layer3.0.downsample.0.weight', 0.1809912472963333), ('layer3.0.downsample.1.weight', 0.01599888876080513), ('layer3.0.downsample.1.bias', 0.015998905524611473), ('layer3.1.conv1.weight', 0.7675238847732544), ('layer3.1.bn1.weight', 0.015998801216483116), ('layer3.1.bn1.bias', 0.01599901169538498), ('layer3.1.conv2.weight', 0.7674823999404907), ('layer3.1.bn2.weight', 0.015998518094420433), ('layer3.1.bn2.bias', 0.01599716953933239), ('layer4.0.conv1.weight', 0.7237126231193542), ('layer4.0.bn1.weight', 0.02262483350932598), ('layer4.0.bn1.bias', 0.022625287994742393), ('layer4.0.conv2.weight', 0.5119794607162476), ('layer4.0.bn2.weight', 0.02262689545750618), ('layer4.0.bn2.bias', 0.02262655831873417), ('layer4.0.downsample.0.weight', 0.36198168992996216), ('layer4.0.downsample.1.weight', 0.02262558601796627), ('layer4.0.downsample.1.bias', 0.02262655831873417), ('layer4.1.conv1.weight', 0.5119829773902893), ('layer4.1.bn1.weight', 0.02262296713888645), ('layer4.1.bn1.bias', 0.02262054942548275), ('layer4.1.conv2.weight', 0.5119814872741699), ('layer4.1.bn2.weight', 0.022626716643571854), ('layer4.1.bn2.bias', 0.02262716181576252), ('fc.weight', 0.07155422121286392), ('fc.bias', 0.0031622741371393204)]\n",
      "Epoch 1/10, loss=2.5434230168660483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747129725.788867   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1747129725.923126   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 tasks\n",
      "update norm: [('conv1.weight', 0.06628871709108353), ('bn1.weight', 0.005991214420646429), ('bn1.bias', 0.005488849710673094), ('layer1.0.conv1.weight', 0.1314864605665207), ('layer1.0.bn1.weight', 0.005558502394706011), ('layer1.0.bn1.bias', 0.006079147569835186), ('layer1.0.conv2.weight', 0.1359984427690506), ('layer1.0.bn2.weight', 0.005466504488140345), ('layer1.0.bn2.bias', 0.005737242288887501), ('layer1.1.conv1.weight', 0.13436788320541382), ('layer1.1.bn1.weight', 0.005838105920702219), ('layer1.1.bn1.bias', 0.0058059729635715485), ('layer1.1.conv2.weight', 0.13594752550125122), ('layer1.1.bn2.weight', 0.005742629989981651), ('layer1.1.bn2.bias', 0.005714145489037037), ('layer2.0.conv1.weight', 0.192329540848732), ('layer2.0.bn1.weight', 0.007882745005190372), ('layer2.0.bn1.bias', 0.008112040348351002), ('layer2.0.conv2.weight', 0.2707218527793884), ('layer2.0.bn2.weight', 0.008122853934764862), ('layer2.0.bn2.bias', 0.008411369286477566), ('layer2.0.downsample.0.weight', 0.06391970813274384), ('layer2.0.downsample.1.weight', 0.0076361712999641895), ('layer2.0.downsample.1.bias', 0.008411369286477566), ('layer2.1.conv1.weight', 0.2719786465167999), ('layer2.1.bn1.weight', 0.008211927488446236), ('layer2.1.bn1.bias', 0.008342920802533627), ('layer2.1.conv2.weight', 0.27195850014686584), ('layer2.1.bn2.weight', 0.008106235414743423), ('layer2.1.bn2.bias', 0.007874363102018833), ('layer3.0.conv1.weight', 0.38371095061302185), ('layer3.0.bn1.weight', 0.011181934736669064), ('layer3.0.bn1.bias', 0.011238997802138329), ('layer3.0.conv2.weight', 0.544594943523407), ('layer3.0.bn2.weight', 0.011298099532723427), ('layer3.0.bn2.bias', 0.010758073069155216), ('layer3.0.downsample.0.weight', 0.12861455976963043), ('layer3.0.downsample.1.weight', 0.011060923337936401), ('layer3.0.downsample.1.bias', 0.010758073069155216), ('layer3.1.conv1.weight', 0.5432344675064087), ('layer3.1.bn1.weight', 0.011563703417778015), ('layer3.1.bn1.bias', 0.011108147911727428), ('layer3.1.conv2.weight', 0.5434024333953857), ('layer3.1.bn2.weight', 0.011698550544679165), ('layer3.1.bn2.bias', 0.011790436692535877), ('layer4.0.conv1.weight', 0.509491503238678), ('layer4.0.bn1.weight', 0.015640312805771828), ('layer4.0.bn1.bias', 0.015398016199469566), ('layer4.0.conv2.weight', 0.3582340180873871), ('layer4.0.bn2.weight', 0.01804266683757305), ('layer4.0.bn2.bias', 0.019106809049844742), ('layer4.0.downsample.0.weight', 0.25159189105033875), ('layer4.0.downsample.1.weight', 0.018228216096758842), ('layer4.0.downsample.1.bias', 0.019106809049844742), ('layer4.1.conv1.weight', 0.35939493775367737), ('layer4.1.bn1.weight', 0.015024445950984955), ('layer4.1.bn1.bias', 0.01532618049532175), ('layer4.1.conv2.weight', 0.3530671000480652), ('layer4.1.bn2.weight', 0.018328027799725533), ('layer4.1.bn2.bias', 0.019816163927316666), ('fc.weight', 0.06068417429924011), ('fc.bias', 0.002681395737454295)]\n",
      "Epoch 2/10, loss=2.497404138247172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747130396.750633   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1747130396.903415   29553 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 tasks\n"
     ]
    },
    {
     "ename": "AioRpcError",
     "evalue": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Socket closed\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-05-13T13:00:41.452363239+03:00\", grpc_status:14, grpc_message:\"Socket closed\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAioRpcError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m loss_array = []\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     model, loss = \u001b[38;5;28;01mawait\u001b[39;00m trainer.epoch(train_loader)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     loss_array.append(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hse/diploma/stoilo2/python_lib/src/stoilo/ddl/dpbgd.py:156\u001b[39m, in \u001b[36mDPBGDTrainer.epoch\u001b[39m\u001b[34m(self, data_loader)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mepoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_loader):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     works = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_create_work(data_loader)\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_aggregate_results(works)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hse/diploma/stoilo2/python_lib/src/stoilo/ddl/dpbgd.py:118\u001b[39m, in \u001b[36mDPBGDTrainer.epoch_create_work\u001b[39m\u001b[34m(self, data_loader)\u001b[39m\n\u001b[32m    115\u001b[39m     tasks.append(task)\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tasks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*(t.submit() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hse/diploma/stoilo2/python_lib/src/stoilo/low_level/task.py:101\u001b[39m, in \u001b[36mStagedTask.submit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msubmit\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> SubmittedTask:\n\u001b[32m     94\u001b[39m     request = task_service_pb2.CreateTaskRequest(\n\u001b[32m     95\u001b[39m         flavor=\u001b[38;5;28mself\u001b[39m._flavor,\n\u001b[32m     96\u001b[39m         call_spec=\u001b[38;5;28mself\u001b[39m._call_spec,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m         redundancy_options=\u001b[38;5;28mself\u001b[39m._redundancy_options,\n\u001b[32m    100\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._create_task(request)\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SubmittedTask(\u001b[38;5;28mself\u001b[39m._connection, task_id=response.task_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hse/diploma/stoilo2/python_lib/src/stoilo/low_level/connection.py:54\u001b[39m, in \u001b[36mConnection._create_task\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connect()\n\u001b[32m     53\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m.network_config.timeout\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stub.CreateTask(request, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hse/diploma/stoilo2/.venv/lib/python3.12/site-packages/grpc/aio/_call.py:327\u001b[39m, in \u001b[36m_UnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m asyncio.CancelledError()\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m _create_rpc_error(\n\u001b[32m    328\u001b[39m             \u001b[38;5;28mself\u001b[39m._cython_call._initial_metadata,\n\u001b[32m    329\u001b[39m             \u001b[38;5;28mself\u001b[39m._cython_call._status,\n\u001b[32m    330\u001b[39m         )\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mAioRpcError\u001b[39m: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Socket closed\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-05-13T13:00:41.452363239+03:00\", grpc_status:14, grpc_message:\"Socket closed\"}\"\n>"
     ]
    }
   ],
   "source": [
    "# Модель: ResNet18 без предварительной тренировки, адаптированная под 10 классов\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "conn = await stoilo.connect('localhost:57010')\n",
    "\n",
    "trainer = DPBGDTrainer(\n",
    "    conn,\n",
    "    model=model,\n",
    "    loss_fn=criterion,\n",
    "    optimizer_class=optim.Adam,\n",
    "    optimizer_kwargs={'lr': learning_rate},\n",
    ")\n",
    "\n",
    "# Цикл обучения\n",
    "loss_array = []\n",
    "for epoch in range(num_epochs):\n",
    "    model, loss = await trainer.epoch(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, loss={loss}\")\n",
    "    loss_array.append(loss)\n",
    "print(*loss_array, sep='\\n')\n",
    "\n",
    "# Оценка качества на тестовом наборе\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Вычисление метрик\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
